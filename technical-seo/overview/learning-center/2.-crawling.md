---
description: >-
  Crawling is how search engines discover new links and process pages it finds
  on the internet.  Here we provide a good overview based only on reliable
  sources.
---

# 2. Crawling

Google's Search Index contains [hundred of billions of webpages and is over 100,000,000 gigibytes \(100k Terabytes\) in size](https://www.google.com/search/howsearchworks/crawling-indexing/).  In order to feed that index, Google uses [Googlebot](https://support.google.com/webmasters/answer/182072?hl=en), the generic name for Google's web crawling infrastructure, to search for new pages, and add to a list of known pages. Bing uses [Bingbot](https://www.bing.com/webmaster/help/which-crawlers-does-bing-use-8c184ec0) as their standard crawler. Both Google and Bing use desktop and mobile User-Agents to crawl the web.

The web has grown into an enormous resource of available information.  It is important to note that Google \(and Bing\) only know about a portion of the web, called the **surface web**.  The surface web is the portion that is accessible publicly by crawlers via web pages.  Other parts of the web are the **deep web**, and **hidden web**. The deep web is estimated to be 4,000 to 5,000 times larger than the surface web.  This [article by the University of Washington](https://guides.lib.uw.edu/c.php?g=342031&p=2300191) details the differences.















## Cloaking

In 2016, Google trained a classification model that was able to accurately detect cloaking 95.5% of the time with a false positive rate of 0.9%.  JavaScript redirection was one of the strongest features, predictive of a positive classification. \([source](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45365.pdf)\)



























